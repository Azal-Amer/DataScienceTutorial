{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "# from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from IPython.display import display\n",
    "import geopy\n",
    "from pathlib import Path  \n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import json\n",
    "import ast\n",
    "import geopandas as gpd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import datetime \n",
    "from matplotlib import colors\n",
    "# import matplotlib.cm as cm\n",
    "from pylab import text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master functions\n",
    "*Functions that I would use everywhere*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Data Collection Functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### County Data\n",
    "**Problem: No comprehensive county dataset had existed that I could access, so I wrote a generator**\n",
    "\n",
    "*NOTE*- Useful source linked in code\n",
    "- Main source used was usa.com\n",
    "- Beautiful soup was used for this. \n",
    "- Essentially, I downloaded the html for a given state's county data, read through it until I got to the beggining of an HTML table\n",
    "- I isolated the table, and then went through its rows and cells. Each time constructing a new corresponding array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The below function takes a given state, and returns a dataframe with every county and its corresponding square milage\n",
    "def county_sqFootage(state):\n",
    "\tstate = re.sub(' ', '-', state)\n",
    "\turl = urllib.request.urlopen('http://www.usa.com/rank/' +state.lower() + '-state--land-area--county-rank.htm')\n",
    "\tpath = url.read()\n",
    "\t# empty list\n",
    "\tdata = []\n",
    "\t# for getting the header from\n",
    "\t# the HTML file\n",
    "\tlist_header = []\n",
    "\tsoup = BeautifulSoup((path),'html.parser')\n",
    "\theader = soup.find_all(\"table\")[1].find(\"tr\")\n",
    "\tfor items in header:\n",
    "\t\ttry:\n",
    "\t\t\tlist_header.append(items.get_text())\n",
    "\t\texcept:\n",
    "\t\t\tcontinue\n",
    "\t# for getting the data\n",
    "\tHTML_data = soup.find_all(\"table\")[1].find_all(\"tr\")[1:]\n",
    "\tfor element in HTML_data:\n",
    "\t\tsub_data = []\n",
    "\t\tfor sub_element in element:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tsub_data.append(sub_element.get_text())\n",
    "\t\t\texcept:\n",
    "\t\t\t\tcontinue\n",
    "\t\tdata.append(sub_data)\n",
    "\t# Storing the data into Pandas\n",
    "\t# DataFrame\n",
    "\tdf = pd.DataFrame(data = data, columns = list_header)\n",
    "\tdf.drop(df.columns[[0]],axis = 1,inplace = True)\n",
    "\tdf.rename(columns = {'County / Population':'County','Land Area â–¼':'Area'}, inplace = True)\n",
    "\t# Removing the section of the data for population information and deleting it\n",
    "\tformatdf= df['County'].str.split(',',expand = True)\n",
    "\tformatdf = formatdf[0]\n",
    "\t# recombining the data\n",
    "\tdf = df['Area'].str.split(' sq', expand = True)\n",
    "\tdf = df.replace({',':''}, regex=True)\t\n",
    "\tdf = df[0]\n",
    "\tdf= pd.concat([df,formatdf],axis=1,join = 'inner')\n",
    "\tdf.columns = ['Area','County']\n",
    "\tdf['County']=df['County'].str.upper()\n",
    "\tdf['Radius'] = df['Area'].astype(float)\n",
    "\tdf['Radius'] = df['Radius'].apply(lambda x: (x/3.1415)**.5)\n",
    "\t# Search Radius is what the Twitter API will end up using\n",
    "\treturn df\t\n",
    "\n",
    "\t# Main code logic copied from Codegeeks https://www.geeksforgeeks.org/convert-html-table-into-csv-file-in-python/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### County Location\n",
    "*While radius and area information is all well and good, I can't do anything if I don't know where the county is*\n",
    "- If the state info exists, then pull it up!\n",
    "- If it doesn't..\n",
    "    - First I pull my county data from a github county location dataset. While it is incomplete for the data I need, it has longitude and lattitude coordinates in one place, which I need\n",
    "    - I open the webpage, convert it to a dataframe, and drop the unnececary columns. Then I isolate only the data relavent to the state I'm searching\n",
    "    - From here I call the previous square footage locator, and merge both dataframes by their County name. \n",
    "    - I save this file in the \"State Information\" folder for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getCountyInfo(state):\n",
    "    \n",
    "    csv_folder = 'State information'\n",
    "    file_path = os.path.join(file_dir, csv_folder, (state+'.csv'))\n",
    "    print((Path.cwd() / csv_folder /(state+'.csv')).exists())    \n",
    "    if((Path.cwd() / csv_folder /(state+'.csv')).exists()):\n",
    "        data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # make a program that uses this dataset https://raw.githubusercontent.com/grammakov/USA-cities-and-states/master/us_cities_states_counties.csv\n",
    "        # to get all the cities in a state\n",
    "        data = pd.read_csv('https://raw.githubusercontent.com/grammakov/USA-cities-and-states/master/us_cities_states_counties.csv',sep='|')\n",
    "        del data['State short'], data['City'],data['City alias']\n",
    "        data.columns = data.columns.str.replace('State full', 'State')\n",
    "        data = data[data.State == state]\n",
    "        data.drop_duplicates(subset='County', keep='first', inplace=True)\n",
    "        del data['State']\n",
    "        coordinates = []\n",
    "        for county in data['County']:\n",
    "            print(county)\n",
    "            locator = geopy.Photon(user_agent=\"myGeocoder\")\n",
    "            location = locator.geocode(county.lower() + ',' + state + ', United States')\n",
    "            # Make a new column in my dataframe with the lat and long\n",
    "            coordinates.append([location.latitude,location.longitude])\n",
    "        data['Coordinates'] = coordinates\n",
    "        area = county_sqFootage(state)\n",
    "        data = pd.merge(data,area, on= 'County', how = 'outer')\n",
    "        data.rename(columns = {'Area':'Area (sq/mi)','Radius':'Radius (mi)'}, inplace = True)\n",
    "        data.to_csv(file_path)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud generator\n",
    "*All this function does is initialize the wordcloud data in one simple place so it's cleaner later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(text,keyword):\n",
    "    file_path = os.path.join(file_dir, 'Keyword State')\n",
    "    mask = np.array(Image.open('cloud.png'))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color='white',\n",
    "    mask = mask,\n",
    "    max_words=3000,\n",
    "    stopwords=stopwords,\n",
    "    repeat=True)\n",
    "    wc.generate(str(text))\n",
    "    name = (state) + \" on the term \" + keyword+ \".png\"\n",
    "    path= os.path.join(file_path,name)\n",
    "    wc.to_file(path)\n",
    "    print(\"Word Cloud Saved Successfully\")\n",
    "    display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullTweets(keyword,geocode,noOfTweet,time):\n",
    "    tweets_list2 = []\n",
    "    for j,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + ' '+ geocode + ' ' + time).get_items()):\n",
    "        tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "        if j == noOfTweet-1:\n",
    "            return tweets_list2\n",
    "    return tweets_list2\n",
    "def percentage(part,whole):\n",
    "    return 100 * float(part)/float(whole)\n",
    "def average(arr):\n",
    "    if len(arr)>0:\n",
    "        return round((sum(arr) / len(arr)),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noOfTweet = 50000\n",
    "state = 'Texas'\n",
    "countyDF =  (getCountyInfo(state))\n",
    "keyword = 'mexican'\n",
    "currentYear = datetime.datetime.now().year\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "file_dir = Path.cwd()\n",
    "\n",
    "cityDict = {}\n",
    "json_folder = 'Keyword State'\n",
    "file_path = os.path.join(file_dir, json_folder, (((state) + \" on the term \" + keyword +'.json')))\n",
    "\n",
    "# extract the county collumn from the countyDF and convert its values to a list\n",
    "countyList = countyDF['County'].values.tolist()\n",
    "usefulTweets = 0\n",
    "users = []\n",
    "county_needed = []\n",
    "tweets_list2 = []\n",
    "total_data = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started!\n",
    "### Failsafe Code\n",
    " - One of the biggest issues that ended up plaguing this project was not that of failed logic, but a failed computer\n",
    " - As such, I had to make sure that at every instance that took any amount of time, the work done could be recovered\n",
    "\n",
    "### What below does\n",
    "- The below code checks to see if a cache file currently exists for the given county, and if it does, checks what counties data has been efficiently scraped from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "\n",
    "# make a function that counts upward from 2000 to the current year\n",
    "# if file_path exists, set the county_needed list equal to all the items from the county list not present in the keys of the json\n",
    "if((Path.cwd() / json_folder /(((state) + \" on the term \" + keyword +'.json'))).exists()):\n",
    "    f = open(file_path, 'r')\n",
    "    cityDict = json.load(f)\n",
    "    checked_counties = cityDict.keys()\n",
    "    # set county_needed equal to the items in county that aren't present in checked_counties\n",
    "    county_needed = countyList-checked_counties\n",
    "    print(county_needed)\n",
    "\n",
    "    print(len(tweet_list))\n",
    "else:\n",
    "    county_needed = countyList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the data\n",
    "NOTE: Due to the way the logic was constructed,it was simply easier to have this large block of code for the loop as opposed to functions. Having the code in one place was more effective for me\n",
    "### Steps\n",
    "#### The Loops: ***County***->Time-> Tweets\n",
    "_Go through every possible tweet in the catagory. This order was decided due to efficiency in scraping._\n",
    "- Accesing the API was probably the most confusing part. Twitter's search has built in search keycodes, so all I needed to do when I requested a batch of tweets, was send a search command with the keycodes for time, and location\n",
    "- Location was determined through the radius of a circle equivalent in area to the county. This proved to be effective in most cases, and was a good enough comprimise while avoiding too much overlap\n",
    "- At the end of each county's iteration, the master dictionary was updated,the cache file was opened, and the cache file would resave. This trick was how I was also able to pause runs. \n",
    "   - Pausing runs proved especially helpful when the searched data could take 30+ hours.\n",
    "\n",
    "### County -> ***Time*** -> Tweets\n",
    "- The county specific dictionary was initialized, saved to cityDict. Additionally, the pandas dataframe that would later be iterated was constructed\n",
    " ### County -> Time -> ***Tweets***\n",
    " - The code loops through every saved tweet from the given batch, and passes it to Vader. Vader returns 4 useful datapoints. \n",
    "    - Polarity: How spread out was the data?\n",
    "    - Negative: How negative was the sentiment for the tweet?\n",
    "    - Positive: How positive was the sentiment for the tweet?\n",
    "    - Neutral: How neutral was the sentiment for the tweet?\n",
    "    - Composite: What was the net sentiment?\n",
    " - Due to some issues with the construction of the composite score, it was actually calculated here using the same averaging algorithm\n",
    " - For later word analysis depending on the composite score, the tweets were then sorted into lists of their respective net sentimen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for county in county_needed:\n",
    "    coordinates = (countyDF.loc[countyDF['County'] == county]['Coordinates'])\n",
    "    coordinates = np.array(coordinates.values.tolist())[0]\n",
    "    coordinates = ast.literal_eval(coordinates)\n",
    "    lat = coordinates[0]\n",
    "    longi = coordinates[1]\n",
    "    miles = [countyDF.loc[countyDF['County'] == county]['Radius (mi)'].values[0]][0]\n",
    "    geocode = 'geocode:' + str(coordinates[0]) + ',' + str(coordinates[1]) + ',' + str(miles) + 'mi'\n",
    "    compScore = []\n",
    "    for year in range(2000,currentYear+1):\n",
    "        timeSearchParams = 'until:' + str(year) + '-12-31' + ' since:' + str(year) + '-01-01'\n",
    "        tweets_list2 = pullTweets(keyword,geocode,noOfTweet,timeSearchParams)\n",
    "        # One of the issues found was that due to the repeated run command, this would have trouble breaking on its own\n",
    "\n",
    "        # Thus I wrote it into a function that would break and return the needed data at the limi\n",
    "        tweets_df = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "        location = county\n",
    "        if location not in cityDict.keys():\n",
    "            cityDict[location] = [0,[],{}]\n",
    "        posList = []\n",
    "        negList = []\n",
    "        neuList = []\n",
    "        compList = []\n",
    "        posCount,negCount,neuCount = 0,0,0\n",
    "        for index,tweet in tweets_df.iterrows():\n",
    "            # location = tweet.user.location\n",
    "            \n",
    "            username = tweet['Username']\n",
    "            tweetText = tweet['Text']\n",
    "            analysis = TextBlob(tweetText)\n",
    "            score = SentimentIntensityAnalyzer().polarity_scores(tweetText)\n",
    "            # SCORE RETURNING IS FINE\n",
    "            neg = score['neg']\n",
    "\n",
    "            neu = score['neu']\n",
    "            pos = score['pos']\n",
    "\n",
    "            comp = score['compound']\n",
    "            # comp variable still works\n",
    "\n",
    "            # ISSUE IS WITH UP HERE!!!\n",
    "            # sortTweetCondition = (username not in users) and (tweetText not in tweet_list)\n",
    "            sortTweetCondition = True\n",
    "            if (sortTweetCondition == True):\n",
    "                # append each score to their respective list\n",
    "                posList.append(pos)\n",
    "                negList.append(neg)\n",
    "                neuList.append(neu)\n",
    "                compList.append(comp)\n",
    "                cityDict[location][0]+=1\n",
    "                usefulTweets+=1\n",
    "                polarity += analysis.sentiment.polarity\n",
    "                if neg > pos:\n",
    "                    negative_list.append(tweetText)\n",
    "                    negative += 1\n",
    "                    negCount+=1\n",
    "                elif pos > neg:\n",
    "                    positive_list.append(tweetText)\n",
    "                    positive += 1\n",
    "                    posCount+=1\n",
    "                elif pos == neg:\n",
    "                    neutral_list.append(tweetText)\n",
    "                    neutral += 1\n",
    "                    neuCount+=1\n",
    "                users.append(username)\n",
    "                tweet_list.append(tweetText)\n",
    "            \n",
    "            # save the cityDict to a json file at directory file_path\n",
    "            \n",
    "        pos = average(posList)\n",
    "        neg = average(negList)\n",
    "        neu = average(neuList)    \n",
    "        comp = average(compList)    \n",
    "        count = posCount+negCount+neuCount\n",
    "        if(len(posList) > 0):\n",
    "            compScore.append(comp)\n",
    "            cityDict[county][2][year] = {'pos':pos,'posCount':posCount,'neg':neg,'negCount':negCount,'neu':neu,'neuCount':neuCount,'count':count,'comp':comp}\n",
    "        # else:\n",
    "        #     print(county + ' returned no tweets')\n",
    "    print(\"For the county of \" + str(county) + \", there were \" + str(cityDict[location][0]) + \" unique tweets\")\n",
    "    cityDict[county][1] = average(compScore)\n",
    "    with open(file_path, 'w') as fp:\n",
    "                json.dump(cityDict, fp)\n",
    "    \n",
    "    # delete the second index of cityDIct\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make a new dictionary identical to countyDict but without the second index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Graphs\n",
    "- The below code essentially mapped out a percentage of sentiment for the tweets. This code gave me a good general idea of the usefulness of my data based on what I may anticipate\n",
    "- I also did another re-save, just in case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = percentage(positive, noOfTweet)\n",
    "negative = percentage(negative, noOfTweet)\n",
    "neutral = percentage(neutral, noOfTweet)\n",
    "polarity = percentage(polarity, noOfTweet)\n",
    "positive = format(positive, '.1f')\n",
    "negative = format(negative, '.1f')\n",
    "neutral = format(neutral, '.1f')\n",
    "# fully expand the array\n",
    "tweet_list = list(set(tweet_list))\n",
    "\n",
    "with open(file_path, 'w') as fp:\n",
    "            json.dump(cityDict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More purely observational data\n",
    "- Reading lengths of dataframes to check for obvious repetition or trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "print('total number: ',len(tweet_list))\n",
    "print('positive number: ',len(positive_list))\n",
    "print('negative number: ', len(negative_list))\n",
    "print('neutral number: ',len(neutral_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup Tweets\n",
    "For the word analysis that followed, it was neccecary to filter out special charecters or any artifacts sent by twitter\n",
    "\n",
    "*NOTE*: This code was copied from the original source, see the main journal entry for my code sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Text (RT, Punctuation etc)\n",
    "\n",
    "#Creating new dataframe and new features\n",
    "tw_list = pd.DataFrame(tweet_list)\n",
    "tw_list[\"text\"] = tw_list[0]\n",
    "\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", x)\n",
    "tw_list[\"text\"] = tw_list.text.map(remove_rt).map(rt)\n",
    "tw_list[\"text\"] = tw_list.text.str.lower()\n",
    "tw_list.head(tweet_list.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new convinience dataframes for the pichart later\n",
    "- Going through each tweet and attaching the term of it's sentiment for easier sorting and dataframe dumping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in tw_list['text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        tw_list.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        tw_list.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        tw_list.loc[index, 'sentiment'] = \"neutral\"\n",
    "        tw_list.loc[index, 'neg'] = neg\n",
    "        tw_list.loc[index, 'neu'] = neu\n",
    "        tw_list.loc[index, 'pos'] = pos\n",
    "        tw_list.loc[index, 'compound'] = comp\n",
    "tw_list.head(10)\n",
    "#Creating new data frames for all sentiments (positive, negative and neutral)\n",
    "tw_list_negative = tw_list[tw_list[\"sentiment\"]==\"negative\"]\n",
    "tw_list_positive = tw_list[tw_list[\"sentiment\"]==\"positive\"]\n",
    "tw_list_neutral = tw_list[tw_list[\"sentiment\"]==\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "#Count_values for sentiment\n",
    "count_values_in_column(tw_list,\"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Appeal Chart\n",
    "- After all the dataframe construction had been done, viewing the pi-chart was one of the best ways to gaugue data reasonability in case I had to stop here   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for Pie Chart\n",
    "pc = count_values_in_column(tw_list,'sentiment')\n",
    "names= pc.index\n",
    "size=pc['Percentage']\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color='white')\n",
    "plt.pie(size, labels=names, colors=['green','blue','red'])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud construction\n",
    "- Moreso than the pichart, the wordcloud was VERY helpful in determining how relavent a keyword was. For this project I needed person centered keywords, so if a synonym for person wasn't high up on the wordcloud, I knew the keyword was bunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = create_wordcloud(tw_list['text'].values,keyword)\n",
    "# save the wordcloud image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mapStateData(state,keyword):\n",
    "    #-------------------------------------\n",
    "    #-------------------------------------\n",
    "    # DATA FRAME SETUP\n",
    "    json_folder = 'Keyword State'\n",
    "    file_dir = Path.cwd()\n",
    "    file_path = os.path.join(file_dir, json_folder, (((state) + \" on the term \" + keyword +'.json')))\n",
    "    fig, (ax1,ax2) = plt.subplots(ncols=2)\n",
    "    shape_dir = os.path.join(file_dir, 'Shapes', 'United States','USA_Counties.shx')\n",
    "    unitedStates = gpd.read_file(shape_dir)\n",
    "    unitedStates = unitedStates[['STATE_NAME','NAME','geometry']].copy()\n",
    "    unitedStates = unitedStates[unitedStates.STATE_NAME == state]\n",
    "    unitedStates['NAME'] = unitedStates['NAME'].str.upper()\n",
    "    unitedStates.rename(columns={'NAME':'County'}, inplace=True)\n",
    "    # Okay turn the dictionary into a dataframe where one column is filled with keys, and the other column is the first value in the dictionary\n",
    "    f = open(file_path, 'r')\n",
    "    state_dict = json.load(f)\n",
    "    stateDF = pd.DataFrame.from_dict(state_dict, orient='index')\n",
    "    stateDF.reset_index(inplace=True)\n",
    "    print(stateDF)\n",
    "    # stateDF = stateDF[[0,1,2]]\n",
    "\n",
    "    stateDF.rename(columns={'index': 'County', 0 :'# of Tweets',1:'score'}, inplace=True)\n",
    "    # stateDF = stateDF.drop(['dead'],1)\n",
    "    # pd.DataFrame(stateDF[\"score\"].to_list(), columns=['score'])\n",
    "    score = []\n",
    "    j=0\n",
    "    for value in stateDF['score']:\n",
    "        if(isinstance(value,float)==False):\n",
    "            stateDF=stateDF.drop(stateDF.index[[j]])\n",
    "        j+=1\n",
    "    j=0\n",
    "    for value in stateDF['score']:\n",
    "        if(isinstance(value,float)==False):\n",
    "            stateDF=stateDF.drop(stateDF.index[[j]])\n",
    "        j+=1\n",
    "    stateDF['score'] = stateDF['score'].astype(float)\n",
    "    unitedStates = pd.merge(unitedStates,stateDF, on= 'County', how = 'outer')\n",
    "    cm1 = colors.LinearSegmentedColormap.from_list(\"MyCmapName\",[\"r\",\"b\"])\n",
    "    #-------------------------------------\n",
    "    #-------------------------------------\n",
    "    # AVERAGE SENT. AND TOTAL POP.\n",
    "    # get the sum of all values in a column\n",
    "    totalTweets = stateDF.iloc[:, 1].sum()\n",
    "    averageSentiment = 0\n",
    "    k=0\n",
    "    for score in stateDF['score']:\n",
    "        # multiply all values in the score column by the corresponding value in the population column\n",
    "        averageSentiment += score * stateDF.iat[k,1]\n",
    "        k+=1\n",
    "    averageSentiment=averageSentiment/totalTweets\n",
    "    averageSentiment=format(averageSentiment, '.2f')\n",
    "    totalTweets = \"{:,}\".format(totalTweets)\n",
    "\n",
    "\n",
    "\n",
    "    #-------------------------------------\n",
    "    #-------------------------------------\n",
    "    # GRAPHING STUFF\n",
    "    norm = colors.TwoSlopeNorm(vmin=-.4, vcenter=0, vmax=.4)\n",
    "    fig.set_size_inches(9, 3)\n",
    "    title = (state + \" on the term \\\"\" + keyword +\"\\\"\" )\n",
    "    fig.suptitle(title, fontsize=15)\n",
    "    fig.tight_layout()\n",
    "    ax1 = unitedStates.plot(ax = ax1,cmap = cm1, norm= norm,edgecolor='black',column = 'score',legend = True, legend_kwds={'label':'Sentiment Score','orientation':'vertical'})\n",
    "    title1 = ('Sentiment')\n",
    "    textX = .1\n",
    "    textY = 0\n",
    "\n",
    "    text(textX, textY,\"Mean State Sentiment: \" + str(averageSentiment), ha='center', va='center', transform=ax1.transAxes,weight='bold')\n",
    "    ax1.set_title(title1)\n",
    "    ax1.set_axis_off()   \n",
    "\n",
    "    max = int(stateDF.iloc[:, 1].max())\n",
    "    print(max)\n",
    "\n",
    "\n",
    "\n",
    "    norm2 = colors.PowerNorm(gamma = .4,vmin=0,vmax=max)\n",
    "    ax2 = unitedStates.plot(ax=ax2,cmap = 'OrRd',norm = norm2,edgecolor='black',column = '# of Tweets',legend = True, legend_kwds={'label':'# of Tweets','orientation':'vertical'})\n",
    "    title2 = ('# Of Tweets from Unique Users')\n",
    "    text(textX, textY,\"Total Tweets: \" + str(totalTweets), ha='center', va='center', transform=ax2.transAxes,weight='bold')\n",
    "    ax2.set_title(title2)\n",
    "    ax2.set_axis_off()\n",
    "    save_dir=os.path.join(file_dir, 'Maps', state)\n",
    "\n",
    "    if(os.path.isdir(save_dir)):\n",
    "        save_name = os.path.join(file_dir, 'Maps', state,keyword +'.png')\n",
    "    else:\n",
    "        # make a directory in the maps folder with the name of the given state\n",
    "        os.mkdir(save_dir)\n",
    "        save_name = os.path.join(file_dir, 'Maps', state,keyword +'.png')\n",
    "    # save the figure to the maps folder inside the folder corresponding to the state\n",
    "\n",
    "    plt.savefig(save_name,dpi=300,facecolor='white', transparent=False)\n",
    "\n",
    "# Bottom left of each map have total average, and the total number\n",
    "# THE MEAN STATE STUFF ISNT WEIGHTED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refresh Maps\n",
    "*In case I updated how the maps looked, I wanted to be able to retroactivley refresh old maps*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataframe\n",
    "*Getting all the data in one easy to access place*\n",
    "- I initialize all the needed path variables, along with the needed cache files.\n",
    "- This code basically just formats the dataframe in a nice way for me to visually understand in case of a bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = 'Keyword State'\n",
    "file_dir = Path.cwd()\n",
    "file_path = os.path.join(file_dir, json_folder, (((state) + \" on the term \" + keyword +'.json')))\n",
    "shape_dir = os.path.join(file_dir, 'Shapes', 'United States','USA_Counties.shx')\n",
    "# Okay turn the dictionary into a dataframe where one column is filled with keys, and the other column is the first value in the dictionary\n",
    "f = open(file_path, 'r')\n",
    "state_dict = json.load(f)\n",
    "stateDF = pd.DataFrame.from_dict(state_dict, orient='index')\n",
    "stateDF.reset_index(inplace=True)\n",
    "# stateDF = stateDF[[0,1,2]]\n",
    "stateDF.rename(columns={'index': 'County', 0 :'# of Tweets',1:'score',2:'Time'}, inplace=True)\n",
    "timeDF = pd.json_normalize(stateDF['Time'])\n",
    "stateDF.drop('Time',axis = 1,inplace= True)\n",
    "maxCount = timeDF[list(timeDF.filter(regex=('count')))].max().max()\n",
    "# sorting the dataframe out for the key information of the given year\n",
    "# What we need to do now is first test to make sure this data can actually be graphed\n",
    "# Then go through all the count columns and identify what the largest value is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Plotting\n",
    "*Making the data actually useful to the common eye*\n",
    "\n",
    "***Time->Map***\n",
    "#### Dataframe work\n",
    "- Additionally, you need to sort the master dataframe for the active year data\n",
    "- Currently unused data is filtered out by fully expanding the cache dictionary, filtering by keywords compliant with the default auto-naming scheme\n",
    "- To get geopandas to properly store values to a county, you need to attach the data to the geopandas shape array\n",
    "\n",
    "#### Key Data Info\n",
    "- For average state sentiment, originally the simple average of all the counties' sentiment was taken. However, I later realized this unfairly weighted counties with nearly no information. Thus a weighted average was taken based on the corresponding mention frequency.\n",
    "    - nan data points were dropped here \n",
    "- The sum of the given year's column was used for a county's mention frequency\n",
    "#### Mapping\n",
    "- The biggest visual issue with the maping ended up simply being generating a meaningful colormap for the data. \n",
    "- Ater asking around, it seemed that the color red seemed to signify negative sentiment, blue for positive, and purple was intuitivley neutral. Unfortunetly matplotlib didn't have a colormap that met this criteria, so I wrote a very simple colormap object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "for year in range(2006,currentYear+1):\n",
    "    year = str(year)\n",
    "    # check if the year is in the dataframe column names\n",
    "    if(len(list(timeDF.filter(regex=year)))>0):\n",
    "        slicetimeDF = timeDF[list(timeDF.filter(regex=('County|' + year+'.comp|' + year+'.count')))]\n",
    "        stateDF = pd.concat([stateDF['County'], slicetimeDF], axis=1)\n",
    "        stateDF.rename(columns= {stateDF.columns[1]:'# of Tweets',stateDF.columns[2]:'score'}, inplace=True)\n",
    "        json_folder = 'Keyword State'\n",
    "        file_dir = Path.cwd()\n",
    "        file_path = os.path.join(file_dir, json_folder, (((state) + \" on the term \" + keyword +'.json')))\n",
    "        fig, (ax1,ax2) = plt.subplots(ncols=2)\n",
    "        shape_dir = os.path.join(file_dir, 'Shapes', 'United States','USA_Counties.shx')\n",
    "        unitedStates = gpd.read_file(shape_dir)\n",
    "        unitedStates = unitedStates[['STATE_NAME','NAME','geometry']].copy()\n",
    "        unitedStates = unitedStates[unitedStates.STATE_NAME == state]\n",
    "        unitedStates['NAME'] = unitedStates['NAME'].str.upper()\n",
    "        unitedStates.rename(columns={'NAME':'County'}, inplace=True)\n",
    "        # Okay turn the dictionary into a dataframe where one column is filled with keys, and the other column is the first value in the dictionary\n",
    "        f = open(file_path, 'r')\n",
    "        score = []\n",
    "        j=0\n",
    "\n",
    "        unitedStates = pd.merge(unitedStates,stateDF, on= 'County', how = 'outer')\n",
    "        cm1 = colors.LinearSegmentedColormap.from_list(\"MyCmapName\",[\"r\",\"b\"])\n",
    "        #-------------------------------------\n",
    "        #-------------------------------------\n",
    "        # AVERAGE SENT. AND TOTAL POP.\n",
    "        # get the sum of all values in a column\n",
    "        totalTweets = stateDF.iloc[:, 1].sum()\n",
    "        averageSentiment = 0\n",
    "        k=0\n",
    "        for score in stateDF['score']:\n",
    "            if(math.isnan(score)==False):\n",
    "                # multiply all values in the score column by the corresponding value in the population column\n",
    "                averageSentiment += score * stateDF.iat[k,1]\n",
    "            k+=1\n",
    "        print(averageSentiment)\n",
    "        averageSentiment=averageSentiment/totalTweets\n",
    "        averageSentiment=format(averageSentiment, '.2f')\n",
    "        totalTweets = \"{:,}\".format(totalTweets)\n",
    "\n",
    "\n",
    "\n",
    "        #-------------------------------------\n",
    "        #-------------------------------------\n",
    "        # GRAPHING STUFF\n",
    "        norm = colors.TwoSlopeNorm(vmin=-.4, vcenter=0, vmax=.4)\n",
    "        fig.set_size_inches(9, 3)\n",
    "        title = (state + \" on the term \\\"\" + keyword +\"\\\"\" + \" in \" + year)\n",
    "        fig.suptitle(title, fontsize=15)\n",
    "        fig.tight_layout()\n",
    "        ax1 = unitedStates.plot(ax = ax1,cmap = cm1, norm= norm,edgecolor='black',column = 'score',legend = True, legend_kwds={'label':'Sentiment Score','orientation':'vertical'})\n",
    "        title1 = ('Sentiment')\n",
    "        textX = .1\n",
    "        textY = 0\n",
    "\n",
    "        text(textX, textY,\"Mean State Sentiment: \" + str(averageSentiment), ha='center', va='center', transform=ax1.transAxes,weight='bold')\n",
    "        ax1.set_title(title1)\n",
    "        ax1.set_axis_off()   \n",
    "        norm2 = colors.PowerNorm(gamma = .4,vmin=0,vmax=maxCount)\n",
    "        ax2 = unitedStates.plot(ax=ax2,cmap = 'OrRd',norm = norm2,edgecolor='black',column = '# of Tweets',legend = True, legend_kwds={'label':'# of Tweets','orientation':'vertical'})\n",
    "        title2 = ('# Of Tweets')\n",
    "        text(textX, textY,\"Total Tweets: \" + str(totalTweets), ha='center', va='center', transform=ax2.transAxes,weight='bold')\n",
    "        ax2.set_title(title2)\n",
    "        ax2.set_axis_off()\n",
    "        save_dir=os.path.join(file_dir, 'Maps', state,keyword)\n",
    "        \n",
    "        if(os.path.isdir(save_dir)):\n",
    "            save_name = os.path.join(file_dir, 'Maps', state,keyword,year +'.png')\n",
    "        else:\n",
    "            # make a directory in the maps folder with the name of the given state\n",
    "            os.mkdir(save_dir)\n",
    "            save_name = os.path.join(file_dir, 'Maps', state,keyword,year +'.png')\n",
    "        # save the figure to the maps folder inside the folder corresponding to the state\n",
    "\n",
    "        plt.savefig(save_name,dpi=300,facecolor='white', transparent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the gif!\n",
    "*Going through all the saved time maps in the keyword directory, and merging them into a gif*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "images = []\n",
    "path = os.path.join(file_dir, 'Maps', state,keyword)\n",
    "files = os.listdir(path)\n",
    "print(files)\n",
    "# for f in files:\n",
    "#     f = f[:-4]\n",
    "for filename in files:\n",
    "\n",
    "    images.append(imageio.imread(os.path.join(path,filename)))\n",
    "save = os.path.join(path,keyword+'.gif')\n",
    "imageio.mimsave(save, images,duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refreshMaps():\n",
    "    folders = (os.path.join(file_dir,'Maps'))\n",
    "    subfolders =os.listdir(folders)\n",
    "    print(subfolders)\n",
    "    for state in subfolders:\n",
    "        print(state)\n",
    "        path = os.path.join(file_dir, 'Maps', state)\n",
    "        files = os.listdir(path)\n",
    "        for f in files:\n",
    "            f = f[:-4]\n",
    "            mapStateData(state,f)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03b31e0dbf5ee774e49906e1a4e99cf39d3bc46df0e15af7b9d16b1e9b24f047"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
